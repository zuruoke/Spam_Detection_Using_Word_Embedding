{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This kernel builds a Neural Network Model using Word Embedding (a paramount feature Natural Language Processing) to classify whether a message is Spam or not.\n",
    "\n",
    "The Dataset consists of 5572 sentences with about 87% labelled as spam message and 13% labelled as ham.\n",
    "\n",
    "P:S I know there is a huge class imbalance which will affect it's generalization to an unseen data, don't worry read on to see how I tackle this.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project workflow includes:\n",
    "\n",
    "- Loading the data  \n",
    "- Examining and understanding the dataset\n",
    "- Preprocess the Text Data\n",
    "- Building an input pipeline\n",
    "- Building an Input Pipeline for the Pretrained Embedding Model\n",
    "- Building an Input Data Pipeline for the Encoded Text Sequences\n",
    "- Building a NN Model using a Model Subclassing API\n",
    "- Training the model\n",
    "- Evaluating the model using Sklearn's confusion_matrix\n",
    "\n",
    "\n",
    "All these will be done with tensorflow 2.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler, TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "import gensim.downloader as api\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "spam = pd.read_csv(\"spam.csv\", encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = spam.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format(v1):\n",
    "    if v1 == \"spam\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "spam[\"v1\"] = spam[\"v1\"].apply(format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    86.593683\n",
       "1    13.406317\n",
       "Name: v1, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam[\"v1\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_spam = spam[spam[\"v1\"] == 0]\n",
    "spam_msg = spam[spam[\"v1\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>Aft i finish my lunch then i go str down lor. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>Ffffffffff. Alright no way I can meet up with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>Just forced myself to eat a slice. I'm really ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>Lol your always so convincing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>Did you catch the bus ? Are you frying an egg ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    v1                                                 v2\n",
       "23   0  Aft i finish my lunch then i go str down lor. ...\n",
       "24   0  Ffffffffff. Alright no way I can meet up with ...\n",
       "25   0  Just forced myself to eat a slice. I'm really ...\n",
       "26   0                     Lol your always so convincing.\n",
       "27   0  Did you catch the bus ? Are you frying an egg ..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_spam[15:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>SIX chances to win CASH! From 100 to 20,000 po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>England v Macedonia - dont miss the goals/team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>Thanks for your subscription to Ringtone UK yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>07732584351 - Rodger Burns - MSG = We tried to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>SMS. ac Sptv: The New Jersey Devils and the De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>Congrats! 1 year special cinema pass for 2 is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>As a valued customer, I am pleased to advise y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1</td>\n",
       "      <td>Urgent UR awarded a complimentary trip to Euro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>Did you hear about the new \\Divorce Barbie\\\"? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>Your free ringtone is waiting to be collected....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1</td>\n",
       "      <td>GENT! We are trying to contact you. Last weeke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1</td>\n",
       "      <td>You are a winner U have been specially selecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1</td>\n",
       "      <td>PRIVATE! Your 2004 Account Statement for 07742...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1</td>\n",
       "      <td>URGENT! Your Mobile No. was awarded å£2000 Bon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "      <td>Todays Voda numbers ending 7548 are selected t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1</td>\n",
       "      <td>Sunshine Quiz Wkly Q! Win a top Sony DVD playe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1</td>\n",
       "      <td>Want 2 get laid tonight? Want real Dogging loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1</td>\n",
       "      <td>You'll not rcv any more msgs from the chat svc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Why haven't you replied to my text? I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>1</td>\n",
       "      <td>Customer service annoncement. You have a New Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1</td>\n",
       "      <td>You are a winner U have been specially selecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1</td>\n",
       "      <td>-PLS STOP bootydelious (32/F) is inviting you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1</td>\n",
       "      <td>BangBabes Ur order is on the way. U SHOULD rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "11    1  SIX chances to win CASH! From 100 to 20,000 po...\n",
       "12    1  URGENT! You have won a 1 week FREE membership ...\n",
       "15    1  XXXMobileMovieClub: To use your credit, click ...\n",
       "19    1  England v Macedonia - dont miss the goals/team...\n",
       "34    1  Thanks for your subscription to Ringtone UK yo...\n",
       "42    1  07732584351 - Rodger Burns - MSG = We tried to...\n",
       "54    1  SMS. ac Sptv: The New Jersey Devils and the De...\n",
       "56    1  Congrats! 1 year special cinema pass for 2 is ...\n",
       "65    1  As a valued customer, I am pleased to advise y...\n",
       "67    1  Urgent UR awarded a complimentary trip to Euro...\n",
       "68    1  Did you hear about the new \\Divorce Barbie\\\"? ...\n",
       "93    1  Please call our customer service representativ...\n",
       "95    1  Your free ringtone is waiting to be collected....\n",
       "113   1  GENT! We are trying to contact you. Last weeke...\n",
       "116   1  You are a winner U have been specially selecte...\n",
       "119   1  PRIVATE! Your 2004 Account Statement for 07742...\n",
       "120   1  URGENT! Your Mobile No. was awarded å£2000 Bon...\n",
       "122   1  Todays Voda numbers ending 7548 are selected t...\n",
       "133   1  Sunshine Quiz Wkly Q! Win a top Sony DVD playe...\n",
       "134   1  Want 2 get laid tonight? Want real Dogging loc...\n",
       "138   1  You'll not rcv any more msgs from the chat svc...\n",
       "146   1  FreeMsg Why haven't you replied to my text? I'...\n",
       "158   1  Customer service annoncement. You have a New Y...\n",
       "159   1  You are a winner U have been specially selecte...\n",
       "163   1  -PLS STOP bootydelious (32/F) is inviting you ...\n",
       "164   1  BangBabes Ur order is on the way. U SHOULD rec..."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_msg[4:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572 sentences, max length: 189\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(spam[\"v2\"])\n",
    "text_sequences = tokenizer.texts_to_sequences(spam[\"v2\"])\n",
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
    "\n",
    "num = len(text_sequences)\n",
    "max_sqlen = len(text_sequences[0])\n",
    "\n",
    "print(\"{:d} sentences, max length: {:d}\".format(num, max_sqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 8921\n"
     ]
    }
   ],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word[0] = \"PAD\"\n",
    "vocab_size = len(word2idx)\n",
    "print(\"vocab size: {:d}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an Input Pipeline for the Pretrained Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix: (8921, 300)\n"
     ]
    }
   ],
   "source": [
    "def build_embedding_matrix(sequences, word2idx, embedding_dim, embedding_file):\n",
    "    if os.path.exists(embedding_file):\n",
    "        F = np.load(embedding_file)\n",
    "        \n",
    "    else:\n",
    "        vocab_size = len(word2idx)\n",
    "        F = np.zeros((vocab_size, embedding_dim))\n",
    "        word_vectors = api.load(EMBEDDING_MODEL)\n",
    "        for word, idx in word2idx.items():\n",
    "            try:\n",
    "                F[idx] = word_vectors.word_vec(word)\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "        np.save(embedding_file, F)\n",
    "    return F\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "DATA_DIR = \"Data\"\n",
    "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"F.npy\")\n",
    "EMBEDDING_MODEL =  \"glove-wiki-gigaword-300\"\n",
    "\n",
    "F = build_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM,EMBEDDING_NUMPY_FILE)\n",
    "\n",
    "print(\"Embedding matrix:\", F.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Build an Input Data Pipeline for the Encoded Text Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Reproducible result\n",
    "np.random.seed(40)\n",
    "tf.random.set_seed(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "cat_labels = tf.keras.utils.to_categorical(spam[\"v1\"], num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = num // 4\n",
    "val_size = (num - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size )\n",
    "BATCH_SIZE = 128\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Model using Model Subclassing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamClassifierModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_sz, embed_sz, input_length, num_filters, kernel_sz, output_sz, run_mode, embedding_weights, **kwargs):\n",
    "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
    "        \n",
    "        if run_mode == \"scratch\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, embed_sz, input_length=input_length, trainable=True)\n",
    "        elif run_mode == \"vectorizer\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, embed_sz, input_length=input_length, weights=[embedding_weights], trainable=False)\n",
    "        elif run_mode == \"fine_tuning\":\n",
    "            self.embedding = tf.keras.layers.Embedding(vocab_sz, embed_sz, input_length=input_length, weights=[embedding_weights], trainable=True)\n",
    "        \n",
    "        self.conv = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_sz, activation=\"relu\")\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
    "        self.dense = tf.keras.layers.Dense(output_sz, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_num_filters = 256\n",
    "conv_kernel_size = 3\n",
    "model = SpamClassifierModel(vocab_size, EMBEDDING_DIM, max_sqlen,conv_num_filters, conv_kernel_size, NUM_CLASSES, \"fine_tuning\", F)\n",
    "\n",
    "\n",
    "model.build(input_shape=(None, max_sqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"spam_detector_weights.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One thing to notice is that dataset is somewhat imbalanced - The dataset consists of about 87% instances of spam and about 13% instances of ham.\n",
    "- The network could achieve an apprx accuracy of 87% by simply predicting the majority class.\n",
    "- To alleviate this problem, I'll set the class weight to implement a harsh penalty for misclassifying the minority class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_WEIGHTS = {0:1, 1:8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 29 steps, validate for 3 steps\n",
      "Epoch 1/5\n",
      "28/29 [===========================>..] - ETA: 1s - loss: 0.0176 - accuracy: 0.9980\n",
      "Epoch 00001: val_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 33s 1s/step - loss: 0.0173 - accuracy: 0.9981 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "28/29 [===========================>..] - ETA: 1s - loss: 0.0101 - accuracy: 0.9994\n",
      "Epoch 00002: val_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 33s 1s/step - loss: 0.0134 - accuracy: 0.9992 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "28/29 [===========================>..] - ETA: 1s - loss: 0.0098 - accuracy: 0.9997\n",
      "Epoch 00003: val_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 33s 1s/step - loss: 0.0097 - accuracy: 0.9997 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "28/29 [===========================>..] - ETA: 1s - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 00004: val_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 33s 1s/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "28/29 [===========================>..] - ETA: 1s - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 00005: val_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 34s 1s/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x225fd335c08>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=10, validation_data=val_dataset, class_weight=CLASS_WEIGHTS, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"spam.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuaracy: 1.000\n",
      "confusion_matrix\n",
      "[[1113    0]\n",
      " [   0  167]]\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = [], []\n",
    "for X_test, Y_test in test_dataset:\n",
    "    Y_test_ = model.predict_on_batch(X_test)\n",
    "    y_test = np.argmax(Y_test, axis=1)\n",
    "    y_test_ = np.argmax(Y_test_, axis=1)\n",
    "    labels.extend(y_test.tolist())\n",
    "    predictions.extend(y_test.tolist())\n",
    "    \n",
    "print(\"test accuaracy: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion_matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 3s 292ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_dataset)\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please if you like my Kernel, give me an upvote"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
